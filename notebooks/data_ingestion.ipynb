{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import zipfile\n",
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CSV files from zipped archive \n",
    "zip_path = '..\\data\\zipped.zip'\n",
    "csv_path = '..\\data\\csv_files'\n",
    "\n",
    "# Create a directory to extract the CSV files\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert CSV files to Parquet files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = '..\\data\\parquet_files'\n",
    "os.makedirs(parquet_path, exist_ok=True)\n",
    "\n",
    "def convert_csv_to_parquet(csv_file, parquet_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.to_parquet(parquet_file)\n",
    "\n",
    "# Iterate through extracted files \n",
    "for root, dirs, files in os.walk(csv_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file_path = os.path.join(root, file)\n",
    "            parquet_file_path = os.path.join(parquet_path, os.path.splitext(file)[0] + '.parquet')\n",
    "            convert_csv_to_parquet(csv_file_path, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Parquet files:\n",
      "benefits.parquet\n",
      "companies.parquet\n",
      "company_industries.parquet\n",
      "company_specialities.parquet\n",
      "employee_counts.parquet\n",
      "industries.parquet\n",
      "job_industries.parquet\n",
      "job_postings.parquet\n",
      "job_skills.parquet\n",
      "salaries.parquet\n",
      "skills.parquet\n"
     ]
    }
   ],
   "source": [
    "# List Parquet files \n",
    "parquet_files = [f for f in os.listdir(parquet_path) if f.endswith('.parquet')]\n",
    "\n",
    "print(\"List of Parquet files:\")\n",
    "for parquet_file in parquet_files:\n",
    "    print(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for benefits.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D68BD96DB0>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 3\n",
      "  num_rows: 29325\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 2391\n",
      "\n",
      "\n",
      "Metadata for companies.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AAE4F0>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 10\n",
      "  num_rows: 11361\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 8304\n",
      "\n",
      "\n",
      "Metadata for company_industries.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AB9E00>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 2\n",
      "  num_rows: 12601\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 2002\n",
      "\n",
      "\n",
      "Metadata for company_specialities.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AAE4F0>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 2\n",
      "  num_rows: 78405\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 1996\n",
      "\n",
      "\n",
      "Metadata for employee_counts.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AB9E00>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 4\n",
      "  num_rows: 14275\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 3060\n",
      "\n",
      "\n",
      "Metadata for industries.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AB9A40>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 2\n",
      "  num_rows: 229\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 2018\n",
      "\n",
      "\n",
      "Metadata for job_industries.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AAE4F0>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 2\n",
      "  num_rows: 44091\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 1984\n",
      "\n",
      "\n",
      "Metadata for job_postings.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AB9A40>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 28\n",
      "  num_rows: 33246\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 16925\n",
      "\n",
      "\n",
      "Metadata for job_skills.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AB98B0>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 2\n",
      "  num_rows: 56591\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 1944\n",
      "\n",
      "\n",
      "Metadata for salaries.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AAE4F0>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 8\n",
      "  num_rows: 13352\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 4836\n",
      "\n",
      "\n",
      "Metadata for skills.parquet:\n",
      "<pyarrow._parquet.FileMetaData object at 0x000001D694AB9A40>\n",
      "  created_by: parquet-cpp-arrow version 15.0.2\n",
      "  num_columns: 2\n",
      "  num_rows: 35\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 1945\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the metadata for each Parquet file\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_file_path = os.path.join(parquet_path, parquet_file)\n",
    "    metadata = pq.read_metadata(parquet_file_path)\n",
    "    print(f\"Metadata for {parquet_file}:\")\n",
    "    print(metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for benefits.parquet:\n",
      "job_id: int64\n",
      "inferred: int64\n",
      "type: string\n",
      "\n",
      "\n",
      "Schema for companies.parquet:\n",
      "company_id: int64\n",
      "name: string\n",
      "description: string\n",
      "company_size: double\n",
      "state: string\n",
      "country: string\n",
      "city: string\n",
      "zip_code: string\n",
      "address: string\n",
      "url: string\n",
      "\n",
      "\n",
      "Schema for company_industries.parquet:\n",
      "company_id: int64\n",
      "industry: string\n",
      "\n",
      "\n",
      "Schema for company_specialities.parquet:\n",
      "company_id: int64\n",
      "speciality: string\n",
      "\n",
      "\n",
      "Schema for employee_counts.parquet:\n",
      "company_id: int64\n",
      "employee_count: int64\n",
      "follower_count: int64\n",
      "time_recorded: double\n",
      "\n",
      "\n",
      "Schema for industries.parquet:\n",
      "industry_id: int64\n",
      "industry_name: string\n",
      "\n",
      "\n",
      "Schema for job_industries.parquet:\n",
      "job_id: int64\n",
      "industry_id: int64\n",
      "\n",
      "\n",
      "Schema for job_postings.parquet:\n",
      "job_id: int64\n",
      "company_id: double\n",
      "title: string\n",
      "description: string\n",
      "max_salary: double\n",
      "med_salary: double\n",
      "min_salary: double\n",
      "pay_period: string\n",
      "formatted_work_type: string\n",
      "location: string\n",
      "applies: double\n",
      "original_listed_time: double\n",
      "remote_allowed: double\n",
      "views: double\n",
      "job_posting_url: string\n",
      "application_url: string\n",
      "application_type: string\n",
      "expiry: double\n",
      "closed_time: double\n",
      "formatted_experience_level: string\n",
      "skills_desc: string\n",
      "listed_time: double\n",
      "posting_domain: string\n",
      "sponsored: int64\n",
      "work_type: string\n",
      "currency: string\n",
      "compensation_type: string\n",
      "scraped: int64\n",
      "\n",
      "\n",
      "Schema for job_skills.parquet:\n",
      "job_id: int64\n",
      "skill_abr: string\n",
      "\n",
      "\n",
      "Schema for salaries.parquet:\n",
      "salary_id: int64\n",
      "job_id: int64\n",
      "max_salary: double\n",
      "med_salary: double\n",
      "min_salary: double\n",
      "pay_period: string\n",
      "currency: string\n",
      "compensation_type: string\n",
      "\n",
      "\n",
      "Schema for skills.parquet:\n",
      "skill_abr: string\n",
      "skill_name: string\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema for each Parquet file\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_file_path = os.path.join(parquet_path, parquet_file)\n",
    "    file = pq.ParquetFile(parquet_file_path)\n",
    "    table = file.read()\n",
    "    print(f\"Schema for {parquet_file}:\")\n",
    "    for field in table.schema:\n",
    "        print(f\"{field.name}: {field.type}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data into PostgreSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x1d694aa8d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SQL database connection\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ln_job_postings')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing benefits.parquet...\n",
      "Inserting benefits.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing companies.parquet...\n",
      "Inserting companies.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing company_industries.parquet...\n",
      "Inserting company_industries.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing company_specialities.parquet...\n",
      "Inserting company_specialities.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing employee_counts.parquet...\n",
      "Inserting employee_counts.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing industries.parquet...\n",
      "Inserting industries.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing job_industries.parquet...\n",
      "Inserting job_industries.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing job_postings.parquet...\n",
      "Inserting job_postings.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing job_skills.parquet...\n",
      "Inserting job_skills.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing salaries.parquet...\n",
      "Inserting salaries.parquet into database...\n",
      "Completed !\n",
      "\n",
      "Processing skills.parquet...\n",
      "Inserting skills.parquet into database...\n",
      "Completed !\n",
      "Total time taken was    240.827 seconds.\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "\n",
    "# Iterate over each Parquet file\n",
    "for parquet_file in parquet_files:\n",
    "    print(f\"\\nProcessing {parquet_file}...\")\n",
    "    \n",
    "    # Load the Parquet file into a PyArrow Table\n",
    "    table = pq.read_table(os.path.join(parquet_path, parquet_file))\n",
    "    table_name = os.path.splitext(parquet_file)[0]\n",
    "\n",
    "    # Convert the PyArrow Table to a pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "    \n",
    "    # Insert the DataFrame into the database\n",
    "    print(f\"Inserting {parquet_file} into database...\")\n",
    "    df.to_sql(name=table_name, con=engine, index=False)\n",
    "    print(f'Completed !')\n",
    "\n",
    "t_end = time()   \n",
    "print(f'Total time taken was {t_end-t_start:10.3f} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
